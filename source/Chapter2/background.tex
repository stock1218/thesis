\chapter{Background}
\label{sec:background}

This chapter will explore the background required to fully understand the motivation of this thesis, the current landscape with regard to integer safety, and important details related to the macros introduced in the new C23 standard. It will cover a short history of C and the C standard to provide the reader with some context on the philosophies of C and how new language features are introduced. This will be followed by a discussion on the development pipeline for safety critical systems, which will also be useful for understanding the requirements of the problem space. The next section will cover integer safety. These sections will further describe the problem space of this thesis. Following this description will be a taxonomy of approaches and their relative effectiveness. The chapter will conclude with relevant details of the new C23 standard that motivated this particular project.

\section{C and the C Standard}

C is a general-purpose imperative procedural compiled programming language. It features important tools like a static type system, raw memory manipulation, portable code generation, and a lightweight runtime.

C as a programming language was initially developed by Dennis Ritchie in the early 1970's at Bell Labs \cite{ritchie}. The purpose of the language was to provide a higher-level programming interface to build operating systems like Unix. C was first standardized in 1989 by the American National Standards Institute (ANSI) and later standardized by the International Organization for Standardization (ISO) in 1990 \cite{ansi}\cite{ISO}. Since then, several more standards have been released to add or remove features from the official language standard. The process of creating a new C standard follows the general ISO standardization process which starts with a proposal, and ends with a publication stage. During this process, there are a series of meetings between stakeholders including the official C standards committee (WG14)\cite{wg14}. The C standards committee is an international group of experts tasked with maintaining, updating, and refining the C standard.

\subsection{Unspecified Behavior and Undefined Behavior}
The C language specification categorizes the behaviors of programs generated by a C compiler. There are some behaviors, however, that are not explicitly defined by the standard. These include \textit{undefined behavior}, \textit{unspecified behavior}, \textit{implementation-defined behavior}, and \textit{locale-specific behavior} \cite{c_standard}. The two behaviors relevant to this thesis are undefined and unspecified behavior.

Undefined behavior occurs when a program is not well formed and exhibits some statement that is ``undefined'' \cite{c_standard}. This can include out of bound accesses, null pointer dereferences, and signed integer overflow. When such a statement is written, the standard allows the compiler to do anything it wants (including making demons shoot out of your nose \cite{nasal_demons}). These statements are often a source of optimization, but can also result in security vulnerabilities.

Unspecified behavior are related to statements that can have multiple outcomes the compiler does not need to document. An example of this is the order of evaluation: a statement with multiple subexpressions does not have to be evaluated in specific or consistent order. This allows compilers again to optimize their instructions to maximize performance.

\section{The Development of Safety Critical Systems}

In many cases, it is unacceptable to have a software fault. This would be the case for many safety critical systems like locomotives, aircraft, cars, or power plants, to name a few. It may not be possible to completely eliminate all bugs, however the software development pipelines for these systems is designed to catch and fix bugs as early as possible. There are several regulatory standards imposed on different systems, and we will not be going over all of them. However, it is important to note a common technique that is commonly employed: manual code review \cite{faa}\cite{do}. That is, when code is written, other developers will need to manually certify that the newly written code does not contain some set of bugs defined in a standard checklist. This will be an important detail as some approaches to code modernization will not take place at the source level, which will need to be reviewed by humans before being certified. 

\section{Integer Safety}

The topic of integer safety has a long and interesting history involving a number of different systems. In this section we will discuss that integer safety is, how violations of integer safety occur, and some real world examples of bugs caused by integer overflow.

\subsection{Integer Representation}

When a integer is declared and initialized, it's underlying value will be represented by a collection of bits. The C standard defines a number of different integer types and can be broken into two different categories: unsigned and signed integers. Unsigned integers represent numbers using raw binary numbers ranging from 0 to $2^{\text{width}(\text{type})} - 1$ where $width(type)$ is the number of bits that represent the type of the integer. Standard unsigned integer types and their widths on x86-64 using the LP64 model include \texttt{unsigned char} (8 bits), \texttt{unsigned short int} (16 bits), \texttt{unsigned int} (32 bits), \texttt{unsigned long int} (64 bits), \texttt{unsigned long long int} (64 bits) \cite{LP64}.

Signed integers are meant to store positive and negative numbers, something you cannot do using an unsigned integer. Standard signed integer types and their widths on x86-64 using the LP64 model include \texttt{signed char} (8 bits), \texttt{short int} (16 bits), \texttt{int} (32 bits), \texttt{long int} (64 bits), \texttt{long long int} (64 bits). The C standard also allows compilers to represent signed integers in three different ways: sign and magnitude, one's complement, and two's complement \cite{c_standard}. Virtually all modern compilers will represent signed integers using two's complement, so only the implications of this representation will be discussed. Two's complement works by treating the most significant bit as a \textit{sign bit}, then giving it a weight of $-2^{N - 1}$ when calculating the decimal value of each bit. Two's complement is the most popular representation of integers because of properties it has to make arithmetic operations easy and does not have issues like a positive and negative 0 representation like one's complement. One important detail however, is the most negative value that two's complement cannot be represented as a positive number using the same number of bits due to the asymmetry of two's complement.

With any type of integer in C, they are represented using a finite set of bits. This means that depending on the number of bits available for a particular integer type, there is a finite set of numbers that can be represented. This leads to the question: what happens when I try to store an integer that needs more bits than is available in a particular type? The answer is truncation, which can result in a misinterpretation of the newly assigned value. The specifics on how the compiler will react to such a situation will depend on the types involved.

When a value that is too large for an \textit{unsigned integer} is stored, \textit{wraparound} occurs. Given an unsigned integer of width 16, only the 16 least significant bits of the value will be stored in the unsigned integer. A property of binary representation is this operation becomes a modulus operation, meaning any value placed in an unsigned destination will have the value $n \bmod width(type)$, where $n$ is the value to place in the unsigned integer and $type$ is the unsigned integer type of the destination.

Interestingly, this behavior is considered defined behavior in the C language specification \cite{c_standard}. This is a point of controversy within the C community but the rational is because representation itself is always a modulus operation, something that some programmers take advantage of, the concept of an \textit{overflow} in an unsigned type has no meaning. Furthermore, this behavior is closer to how hardware would perform, wrapping around when the maximum or minimum value is reached.

\subsection{Integer Overflow}

An \textit{integer overflow} occurs when an operation involving a \textit{signed integer} results in a value that is too large or small to be represented in the resultant type. The C standard considers integer overflow undefined behavior, which has a number of implications as mentioned earlier in this chapter. Operations that can cause overflow are listed in table 2.1.
\begin{table}[h!]
\centering
\begin{tabular}{|c|l|}
\hline
\textbf{Operator} & \textbf{Description} \\
\hline
\texttt{+}        & Addition \\
\texttt{-}        & Subtraction \\
\texttt{*}        & Multiplication \\
\texttt{/}        & Division \\
\texttt{\%}       & Modulo \\
\texttt{<<}       & Left shift\\
\hline
\texttt{+=}       & Compound addition assignment \\
\texttt{-=}       & Compound subtraction assignment \\
\texttt{*=}       & Compound multiplication assignment \\
\texttt{/=}       & Compound division assignment \\
\texttt{\%=}      & Compound modulo assignment \\
\texttt{<<=}      & Compound left shift assignment \\
\hline
\texttt{++}       & Pre/post-increment \\
\texttt{--}       & Pre/post-decrement \\
\hline
\end{tabular}
\caption{Operations that can Overflow}
\end{table}

Something interesting to note is that division and modulo operations can overflow. This is due to the asymmetry of two's complement as previously mentioned, where the most negative value of two's complement cannot be represented as a positive number. Thus, any operation that can switch the sign (like division and modulo) on the smallest possible value will result in an overflow. Because this overflow depends on the way integers are represented, technically this subset of operations become safe when a different representation is used, like one's complement.

\subsection{Integer Conversion}

Integers can be converted from one integer type to another explicitly through a type cast, or implicitly within a mixed-typed statement. For example, an implicit cast could be inserted by the compiler in an arithmetic expression, an assignment, or even a function call. The intention of these rules is to make it easy for programmers to write mixed-typed statements without having to worry about explicit casts, so long as they remember the rules that govern them.

There are three concepts that are outlined in the C standard regarding integer conversions: \textit{integer conversion rank}, \textit{integer promotion}, \textit{usual arithmetic conversions} and \textit{default arithmetic conversions}. These concepts are explained in the following subsections:

\subsubsection{Integer Conversion Rank}

Each integer type is assigned a type of \textit{rank} that is used to govern how conversions are made between different types. The rank assigned to a type is determined by the rules in Table 2.2 \cite{c_standard}.

\begin{table}[h!]
\centering
\begin{tabular}{|c|p{13cm}|}
\hline
\textbf{Rule} & \textbf{Description} \\
\hline
1 & The ranks of all signed integer types are different and increase with their precision.\\
\hline
2 & The ranks of all signed integer types equal the ranks of the corresponding unsigned integer types. \\
\hline
3 & The rank of any standard integer type is greater than the rank of any extended integer type or bit-precise integer type of the same size.\\
\hline
4 & The rank of \texttt{char} equals the rank of \texttt{signed char} and the rank of \texttt{unsigned char}. \\
\hline
5 & The rank of \texttt{bool} is less than the rank of any other standard integer type. \\
\hline
6 & The rank of any enumerated type equals the rank of its compatible integer type. \\
\hline
7 & Ranking is transitive.\\
\hline
8 & The rank of a bit-precise signed integer type shall be greater than the rank of any standard integer type with less width or any bit-precise integer type with less width. \\
\hline
9 & The rank of any bit-precise integer type relative to an extended integer type of the same width is implementation-defined. \\
\hline
10 & Any aspects of relative ranking of extended integer types not covered above are implementation-defined. \\
\hline
\end{tabular}
\caption{C23 Integer Conversion Rank Rules}
\end{table}

The purpose of these rules becomes clear in the next section.

\newpage
\begin{table}[h!]
\centering
\begin{tabular}{|p{4cm}|p{10cm}|}
\hline
\textbf{Condition} & \textbf{Conversion Rule} \\
\hline
Target type can represent the value & The value is unchanged. \\
\hline
Target type is \texttt{unsigned} and cannot represent the value & The value $2^b$, where $b$ is the number of value bits in the target type, is repeatedly added or subtracted to bring the value into range. In other words, unsigned integers implement modulo arithmetic. \\
\hline
Target type is \texttt{signed} and cannot represent the value & The result is implementation-defined behavior (which may include raising a signal). \\
\hline
\end{tabular}
\caption{Integer Conversion Rules in C23}
\end{table}
\newpage

\subsubsection{Integer Promotion}

Integer promotion is the implicit conversion of any type with a rank less than an \texttt{int} or \texttt{bool} to an \texttt{int}. Note that the promoted type can be signed or unsigned. Furthermore, these promotions are intended to preserve the value and sign of the initial value. The rules that determine how promotion is handled can be found in Table 2.3.

Integer promotion only apply to the following:
\begin{itemize}
    \item Usual arithmetic conversions
    \item Default argument promotions
    \item Operands of the \texttt{++} and \texttt{--} operators
    \item Operand of the \texttt{\textasciitilde} operator
    \item Both operands of the \texttt{<<} and \texttt{>>} operators
\end{itemize}

The next section will cover the case most relevant to this project, usual arithmetic conversions.

\subsubsection{Usual Arithmetic Conversions and Default Argument Promotions}

When a mix-typed statement is written, the compiler will need to calculate the \textit{common real type} of the operation itself, which will be used to complete the operation. Operations that would trigger a conversion include binary arithmetic, relational operations, bitwise operations, and the conditional operator. The rules for how the common real type is generated can be found in appendix \ref{appendix:common_real_type}.


As mentioned in the integer promotion subsection, default argument promotions can also trigger integer promotion. This occurs when the arguments are passed to a variadic function and are matched to the ellipsis parameter.

Now will these concepts explained, it is important to understand how these rules can be a source of error. The implication is that these casts can result in a loss of value (i.e. conversion where the magnitude of original value cannot be preserved) or a loss of sign (i.e. converting from a signed value to an unsigned value will mean negative values are no longer represented).

Consider the following example \cite{secure_coding} :

\begin{center}
\parbox{0.9\linewidth}{
\texttt{unsigned int ui = ULONG\_MAX;\\
signed char c = -1;\\
if (c == ui) \{\\
\hspace*{2em}puts("Why is -1 = 4,294,967,295???");\\
\}
}}
\end{center}

In the code above, the signed char \texttt{c} is compared to signed int \texttt{ui}. As a result, \texttt{c} is promoted to an \texttt{unsigned int} and now represents a very large unsigned value.

\subsection{Case Studies}

There have been a number of real-world bugs that have been caused by integer overflow and wraparound. One of the most famous examples is the Ariane 5 rocket disaster. The disaster was caused by an integer overflow that resulted erroneous horizontal velocity values, leading to the rocket to self-destruct \cite{a5}. The damage exceeded \$370 million dollars \cite{a5_cost}.

Another prominent example was the Therac-25 radiation therapy machine. This machine was meant to deliver a dosage of radiation to patients to treat illnesses like cancer. However, there were a number of software related issues including an arithmetic overflow that caused certain safety checks to be bypassed \cite{therac}. The result of these errors resulted in radiation doses that exceeded 100 times the intended amount in a smaller area. Patients developed radiation burns and three individuals lost their lives.

One thing to note is this issue is not constrained to C. The overflow in the Ariane 5 was in Ada. In all, these examples should demonstrate the consequences of poor integer safety.

\section{Previous Approaches to Ensuring Integer Safety}

Given how long integer safety has been an issue for C programmers, there are a number of approaches that have been adopted by the community. They each have pros and cons and are discussed in the following subsections. Note that from this point on we only consider approaches to integer safety that do not involve arbitrary precision types.

\subsection{General Approaches for Detecting Overflow and Wraparound}

There are a number of general approaches that can be used to detect when overflow or wraparound occurs that involve modifying or instrumenting code to catch these cases. These approaches don't depend on any particular build system or language as they are implemented by the programmer manually. These are discussed below:

\subsubsection{Pre and post condition testing}
The most basic approach to detecting overflow and wraparound involve introducing checks before and after the risky operation. For example, take the following operation:

\begin{center}
\parbox{0.9\linewidth}{
\texttt{int a, b, c;\\
// initialize a and b\\
c = a + b;}
}
\end{center}

The line \texttt{c = a + b} may overflow. Using a precondition check, we can test if the current values of \texttt{a} and \texttt{b} will indeed trigger an overflow before performing the operation. This could be written like so \cite{secure_coding}:

\begin{center}
\parbox{0.9\linewidth}{
\texttt{int a, b, c;\\
// initialize a and b\\
if ((b > 0 \&\& a > INT\_MAX - b) ||\\
\hspace*{2em}(b < 0 \&\& a < INT\_MIN - b)) \{\\
\hspace*{1.5em}// OVERFLOW\\
\} else \{\\
\hspace*{1.5em}c = a + b;\\
\}}
}
\end{center}
In english, the code above will test that there is room between the current value of \texttt{a} and the upper and lower bounds of signed integer representation.

There is a notion of a post-condition test as well. Below is an example of how one would right a post-condition check for wraparound for two unsigned integers:

\begin{center}
\parbox{0.9\linewidth}{
\texttt{unsigned a, b, c;\\
// initialize a and b\\
c = a + b;\\
if (c < a) \{\\
\hspace*{1.5em}// WRAPAROUND\\
\}}
}
\end{center}

Although easy to implement, there are a number of issues with this approach. One is related to performance: constantly checking for overflow can introduce overhead and damage performance. Another is it becomes difficult to write multi-operation arithmetic statements like \texttt{int c = a + b + c}, which would need to perform two operations and thus two checks. Although this is a simple example, these statements can get quite complex and difficult to rewrite. This issue is further complicated by the fact that the order of evaluation is unspecified behavior and may vary between different compilers.

\subsubsection{Using Status Flags}

In x86, there exist instructions that will perform a type of postcondition check for overflow once an operation is performed. The overflow (\texttt{OF}) and carry (\texttt{CF}) flags will be set after an instructions like \texttt{add}, \texttt{sub}, and \texttt{mul} if overflow or wraparound occurs \cite{x86}. Instructions like \texttt{jo} and \texttt{jc} can then perform jumps if these bits are set to an overflow or wraparound handler.

This approach also introduces overhead similar to the general checks previously mentioned. The performance issue is actually worse in this case, since the compiler likely won't be able to optimize this assembly. Furthermore, this approach is not portable and depends on both the architecture of the system and the compiler extensions supported to write inline assembly (the most likely approach to injecting these instructions). It again suffers from issue of potentially writing overly complex statements to check multi-operation statements.

\subsubsection{Casting the result to a larger type}

A simple solution to detecting overflow and wraparound in addition and multiplication is to first cast the operands to a larger type, perform the operation and check for overflow, then downcast to the original resultant type. For addition, any result can be represented by $w+1$ bits, where $w$ is the width of the largest operand. This rule is the same for multiplication, however each operand needs to be at least $2w+1$.

Once the operation is complete, the result can then be range checked in a type of postcondition check, then downcasted to the intended type if no overflow or wraparound occurred. This approach is simple however it does depend on the compiler since C does not guarantee any standard type is larger than any other standard type.

\subsection{Language-based Approaches}

C is a very old language and its ability to produce integer arithmetic errors largely stems from this fact. Since C was created, many other languages have been developed and have addressed the issue of integer safety in different ways. The following are several notable examples.

\subsection{Python}

Python was initially created as a learning language by Guido van Rossum in 1991 as a type of learning language. It is a dynamically typed, mutli-paradigm scripting language. In Python, integers are represented using arbitrary precision, meaning integers can have as many bits as they need to be represented granted there is enough memory on the system \cite{python_types}. As a result, for integer types there is no notion of an ``overflow" or ``wraparound".

Interestingly, there is still an \texttt{OverflowError} exception that can be raised if an integer is too large to be represented \cite{python_overflow}. This error cannot occur for integers however, and instead a \texttt{MemoryError} is raised. According to the documentation it may still be raised if an integer is outside some range due to historical reasons. For completeness, floating-point operations \textit{can} overflow, which is due to a lack of standardization for handling floating-point exceptions in C.

\subsubsection{Java}

Integers are represented in Java using two's complement arithmetic and will wraparound when a value is too large to be represent a particular value \cite{java_int}. To address this, Java 8 provides a set of functions that can perform operations and either return the result of the operation or throw an exception. These functions are: \texttt{addExact}, \texttt{subtractExact}, \texttt{multiplyExact}, \texttt{incrementExact}, \texttt{decrementExact}, and \texttt{negateExact} \cite{java_exact}.

Another option that Java provides is the \texttt{BigInteger} type, which allows for arbitrary precision integers \cite{bigint}.

\subsubsection{Rust}

Rust is often used as a modern replacement for C and C++ given it's capability as a systems programming language while still supporting many safety mechanisms. Of these features is integer safety. When a Rust program is compiled in debug mode, checks are inserted in the binary to detect integer overflow \cite{rust_book}. When an overflow is detected, a \textit{panic} is triggered. An important detail here is that these checks are removed when a rust binary is compiled in \textit{release} mode. To address this, Rust provides a set of \texttt{checked*} functions to detect integer overflow due to arithmetic operations \cite{rust_checked}. These will ensure that checks are performed even in release builds. Downsides of this approach are rewrites of complex operations can be tricky and there is a performance overhead.

Perhaps the most dangerous aspect of this is what most developers think Rust is capable of. Namely, that as long as the \texttt{unsafe} keyword is not being used, integer overflow is not something to worry about. This is simply not the case, as integer safety is still an issue in release builds.

\subsection{Checked Arithmetic Libraries}

Another approach to making C arithmetic safe is to use an external library to perform arithmetic in a safe manner. The exact approach for how code should be written or rewritten will depend on the library, and can also issues related to compatibility. The following subsections cover some modern safe arithmetic libraries for C and C++.

\subsubsection{SafeInt}

The SafeInt library approaches integer safety by providing functions to explicitly cast integers and perform arithmetic operations. This is a great approach since these are the sources of integer errors. In effect this library provides safe functions for assignment, casting, comparison, arithmetic, and logical operations. Below is an example given in their documentation:

\begin{flushleft}
\begin{minipage}{\linewidth}
\texttt{int main()\\
\{\\
\hspace*{1.5em}int divisor = 3;\\
\hspace*{1.5em}int dividend = 6;\\
\hspace*{1.5em}int result;\\
\hspace*{1.5em}bool success = SafeDivide(dividend, divisor, result);\\
\hspace*{1.5em}success = SafeDivide(dividend, 0, result);\\
\}
}
\end{minipage}
\end{flushleft}


This library was originally written for C++, however now supports C via the \texttt{safe\_math.h} and \texttt{safe\_math\_impl.h} header files.

\subsubsection{Intel Safe Arithmetic}

The Intel Safe Arithmetic library approaches the issue of integer safety via compile-time checks. It provides safe versions of common types, putting them in a type of ``safe arithmetic environment". This means any operations performed on these new types are checked for unsafe behavior including signed overflow and unsigned wraparound.

This approach has a massive benefit: there is no runtime overhead. Since these checks are performed at compile-time and any potential errors generate warnings, no instrumentation or handling needs to be inserted to detect and handle overflows. Furthermore, so long as the types are changed existing code can remain relatively similar and reap the benefits of integer safety. The downside to this library is it only supports C++.

\subsubsection{safe\_numerics}

The safe\_numerics library uses a hybrid compile and runtime approach. Similar to Intel's Safe Arithmetic library, new safe types are provided that can be used like any other builtin type. When enough information can be determined that an overflow may occur at compile-time, a warning is generated. If not, and an overflow occurs during runtime, the overflow is handled via runtime exception that the developer can handle. This has the added benefit of being more complete than only using a compile-time approach, however there is a small runtime overhead due to the added runtime checks. This library is also only written for C++.

\subsection{Compiler-based Approaches}

Compiler engineers have developed techniques to automatically mitigate integer overflow and wraparound vulnerabilities. These include inserting instrumentation when an overflow is detected and providing functions for developers to easily perform precondition tests.

\subsubsection{The \texttt{-ftrapv} flag}

Compilers like GCC and Clang provide a number of compiler flags that instruct the compiler on how to deal with integer overflow and wraparound. The \texttt{-ftrapv} flag will instruct the compiler to insert instrumentation to detect and handle signed integer overflow \cite{ftrapv}.

The downsides of this approach is there is again an overhead to inserting these checks and this flag is known for having some implementation issues. These issues include the conditional branches used to handle the trap (which can be expensive) and will only detect overflow for a subset of operations that can produce an overflow. Another is this flag will not detect unsigned wraparound, which may be unintended behavior.

Furthermore, this approach also has the limitation of not inserting changes at the source level. This could be an issue for development pipelines that need to verify changes made by any automatic tools including passes by a compiler like a safety critical system.

One interesting note is that GCC seems to implement handling by calling \texttt{abort()}, which is not always the correct approach to handling overflow or any kind of error. Clang however does have a \texttt{-ftrapv-handler} flag that allows developers to specify a function to call when an error is detected \cite{clang-ftrapv}.

\subsubsection{Undefined Behavior and Integer Overflow Sanitizers}

Both GCC and Clang provide an Undefined Behavior Sanitizer (UBSan), which will instrument the compiled binary to detect a number of undefined behaviors in a program \cite{gcc-ubsan}\cite{clang-ubsan}. This is similar to using the \texttt{-ftrapv} flag but spans a number of other patterns that can result in undefined behavior. This includes, out-of-bounds array subscripts, bitwize shifts that are too large, null-pointer dereferences, and signed integer overflow.

UBSan is slightly better than using the \texttt{-ftrapv} flag since it can also be configured to check for unsigned integer overflow. Besides this benefit, this approach suffers from the same issues using \texttt{-ftrapv} incurs.

\subsubsection{Overflow Checking Functions}

GCC and Clang offer language extensions for checking if an arithmetic operation will overflow\cite{gcc-lang-ext}\cite{clang-lang-ext}. The general structure of these functions are to pass both arguments and the destination for the operation to one of these functions. This function will then both perform the operation and check for overflow using the promoted operands. It will then return 1 if an overflow is detected and 0 otherwise. The programmer is meant to check the return value of this operation then handle the case where an overflow is detected.

For the sake of space, only a subset of Clang's extensions are listed below:
\begin{tabbing}
\texttt{bool \_\_builtin\_add\_overflow(type1 x, type2 y, type3 *sum);}\\
\texttt{bool \_\_builtin\_sub\_overflow(type1 x, type2 y, type3 *diff);}\\
\texttt{bool \_\_builtin\_mul\_overflow(type1 x, type2 y, type3 *prod);}
\end{tabbing}

With regards to making source-level checks to detect integer overflow, this is the best approach. There are still downsides around readability and ensuring programmers call these functions and handle errors correctly, which are shared with issues pre and post condition checks have. Furthermore, these are compiler \textit{extensions}, and are not part of the C standard. This means there is a dependence on both compiler support and how the compiler implements these functions (the names and signatures of these functions are very different between Clang and GCC).

Notably, both GCC and Clang offer signed and unsigned versions of these functions, and implement the generic check function (i.e. \texttt{builtin\_add\_overflow(type1 x, type2, type3, *sum)} such that it will detect both signed integer overflow and unsigned integer wraparound.

\subsection{Testing and Analysis}

Software testing and using automated or manual analysis is likely the most common approach to avoiding integer arithmetic errors. These techniques can range from simple unit tests to complex guided fuzzing or even formal verification. Because the range of approaches is quite wide and the pros and cons of this approach really depend on the technique applied here. The approaches we will consider are static analysis, testing, and manual review.

Static analysis tools can be quite powerful and will generally involve running the tool, then rewriting code to prevent or remove a line of source that could be unsafe \cite{secure_coding}. The benefit is that these tools can integrate nicely in a development environment, however the main downside will be false positives and negatives. Static analysis tools are generally more prone to producing false positives. When the number of false positives is reduced, this may also result in false negatives. It is very difficult to build a static analysis tool that is both sound and complete, so developers should not rely solely on this approach.

Testing is a good practice for any software development, and can quickly locate bugs before they become bigger issues. Unfortunately, this approach cannot make guarantees about how code will behave in the field and it is difficult to ensure all possible inputs are tested.

Finally, the last approach we will consider is a manual source code audit. This is also common practice, and can be quite effective for locating bugs. However this approach may fail to locate bugs due to the error-prone humans it relies on.

The biggest downside to all of these approaches are how much more involved humans must be, which suffers from issues related to human error and the cost of labor.

\section{The C23 Approach}

In December 2023, the C23 standard was officially published \cite{c_standard}. Among the changes to the C standard was a set of checked arithmetic functions for testing if certain operations would overflow \cite{ckd_arith}. These functions were essentially inspired by the language extensions GCC and Clang have for detecting integer overflow and wraparound. As a result, the GCC and Clang compilers have essentially just provided implementations that wrap their existing extensions.

\subsection{The ckd\_add, ckd\_sub, and ckd\_mul Macros}

C23 defines the set of checked arithmetic macros in the \texttt{<stdckdint.h>} as follows:

\begin{flushleft}
\texttt{bool ckd\_add(type1* result, type2 a, type3 b);}\\
\texttt{bool ckd\_sub(type1* result, type2 a, type3 b);}\\
\texttt{bool ckd\_mul(type1* result, type2 a, type3 b);}
\end{flushleft}

Each function will perform the operation as if an infinite-precision model is being used, then return \texttt{1} if an overflow occurs. Note that these are ``type-generic" macros and can take several types. The standard says that ``\texttt{type2} and \texttt{type3} shall be any integer type other than ``plain” \texttt{char}, \texttt{bool}, a bit-precise integer type, or an enumerated type, and they need not be the same. \texttt{*result} shall be a modifiable lvalue of any integer type other than ``plain” \texttt{char}, \texttt{bool}, a bit-precise integer type, or an enumerated type" \cite{ckd_arith}.

\subsection{Open Questions}

These macros are a welcome addition to the C programming language and hopefully will be used to ensure critical operations are checked. However, there are still some open questions about this approach, which are discussed below:

\subsubsection{Should these functions work for signed and unsigned types?}
As we have seen in the language extensions provided by GCC and Clang, signed and unsigned macros have been provided to be specific with the type of behavior that is being detected. So far there is no distinction here in the C23 standard, and the standard does not discuss this detail. Tests have revealed that these macros do detect unsigned wraparound, however this is likely implementation defined behavior and could result in issues if different compilers are used.

\subsubsection{How do you ensure the correct types are used?}

The type of the destination and operands are very important for determining if an operation will result in an overflow or wraparound. Furthermore, C has a number of complex rules that govern implicit casts as mentioned previously. The concern here is should the resultant type be determined exclusively by the developer? If not, could the compiler be used to determine if there is going to be an overflow?

\subsubsection{How is a two argument signature going to change things?}

There is a proposal to move from a three argument macro to a two argument macro that just contains the operands of an argument. This is supposedly a future plan but can also suffer from a similar issue to the typing issue: how will the compiler determine the resultant type and thus correctly detect overflow?

\subsubsection{What about other operations and types?}

A final question is how will these macros evolve to detect overflow in other operations. The restrictions the C23 standard places on the types of the destination and operands is quite limiting, and makes these macros ineffective against a very large proportion of operations that may overflow.